{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/datastorage/Phong/cifar10/cifar-10-batches-py\n"
     ]
    }
   ],
   "source": [
    "cd /media/datastorage/Phong/cifar10/cifar-10-batches-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix for multi_gpu_model prediction time longer\n",
    "from keras.layers import Lambda, concatenate\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def multi_gpu_model(model, gpus):\n",
    "    if isinstance(gpus, (list, tuple)):\n",
    "        num_gpus = len(gpus)\n",
    "        target_gpu_ids = gpus\n",
    "    else:\n",
    "        num_gpus = gpus\n",
    "        target_gpu_ids = range(num_gpus)\n",
    "\n",
    "    def get_slice(data, i, parts):\n",
    "        shape = tf.shape(data)\n",
    "        batch_size = shape[:1]\n",
    "        input_shape = shape[1:]\n",
    "        step = batch_size // parts\n",
    "        if i == num_gpus - 1:\n",
    "            size = batch_size - step * i\n",
    "        else:\n",
    "            size = step\n",
    "        size = tf.concat([size, input_shape], axis=0)\n",
    "        stride = tf.concat([step, input_shape * 0], axis=0)\n",
    "        start = stride * i\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    all_outputs = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        all_outputs.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU,\n",
    "    # each getting a slice of the inputs.\n",
    "    for i, gpu_id in enumerate(target_gpu_ids):\n",
    "        with tf.device('/gpu:%d' % gpu_id):\n",
    "            with tf.name_scope('replica_%d' % gpu_id):\n",
    "                inputs = []\n",
    "                # Retrieve a slice of the input.\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_i = Lambda(get_slice,\n",
    "                                   output_shape=input_shape,\n",
    "                                   arguments={'i': i,\n",
    "                                              'parts': num_gpus})(x)\n",
    "                    inputs.append(slice_i)\n",
    "\n",
    "                # Apply model on slice\n",
    "                # (creating a model replica on the target device).\n",
    "                outputs = model(inputs)\n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "\n",
    "                # Save the outputs for merging back together later.\n",
    "                for o in range(len(outputs)):\n",
    "                    all_outputs[o].append(outputs[o])\n",
    "\n",
    "    # Merge outputs on CPU.\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for name, outputs in zip(model.output_names, all_outputs):\n",
    "            merged.append(concatenate(outputs,\n",
    "                                    axis=0, name=name))\n",
    "        return Model(model.inputs, merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Stop training on val_acc\n",
    "class EarlyStoppingByAccVal(Callback):\n",
    "    def __init__(self, monitor='val_acc', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current >= self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "#Save large model using pickle formate instead of h5            \n",
    "class SaveCheckPoint(Callback):\n",
    "    def __init__(self, model, dest_folder):\n",
    "        super(Callback, self).__init__()\n",
    "        self.model = model\n",
    "        self.dest_folder = dest_folder\n",
    "        \n",
    "        #initiate\n",
    "        self.best_val_acc = 0\n",
    "        self.best_val_loss = sys.maxsize #get max value\n",
    "          \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_acc = logs['val_acc']\n",
    "        val_loss = logs['val_loss']\n",
    "\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            \n",
    "            # Save weights in pickle format instead of h5\n",
    "            print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "            weigh= self.model.get_weights()\n",
    "\n",
    "            #now, use pickle to save your model weights, instead of .h5\n",
    "            #for heavy model architectures, .h5 file is unsupported.\n",
    "            fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "            pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "            fpkl.close()\n",
    "            \n",
    "#             model.save('tmp.h5')\n",
    "        elif val_acc == self.best_val_acc:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss=val_loss\n",
    "                \n",
    "                # Save weights in pickle format instead of h5\n",
    "                print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "                weigh= self.model.get_weights()\n",
    "\n",
    "                #now, use pickle to save your model weights, instead of .h5\n",
    "                #for heavy model architectures, .h5 file is unsupported.\n",
    "                fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "                pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "                fpkl.close()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#501\n",
    "#MUL 3 InceptionResNetV2\n",
    "# from keras.applications import InceptionV3\n",
    "# from keras.applications import Xception\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, GRU, Reshape, Concatenate,Bidirectional\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "f3_base_1 = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(299,299,3))  \n",
    "# for layer in f3_base.layers:\n",
    "#     layer.trainable = False \n",
    "f3_x_1 = f3_base_1.output\n",
    "f3_x_1 = GlobalAveragePooling2D()(f3_x_1)\n",
    "\n",
    "f3_x_1 = Reshape([1,1536])(f3_x_1)  \n",
    "f3_x_1 = GRU(2048, \n",
    "            return_sequences=False,                       \n",
    "#             dropout=0.8                                     \n",
    "            input_shape=[1,1536])(f3_x_1)\n",
    "\n",
    "#Regularization with noise\n",
    "f3_x_1 = GaussianNoise(0.1)(f3_x_1)\n",
    "\n",
    "f3_x_1 = Dense(2048, activation='relu')(f3_x_1)\n",
    "f3_x_1 = Dense(10, activation='softmax')(f3_x_1)\n",
    "model_3_1 = Model(inputs=[f3_base_1.input],outputs=[f3_x_1])\n",
    "\n",
    "print(model_3_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_1.load_weights(os.path.join('checkpoints', 'Cifar10_IcpResNetV2_LRG299_Mul_GRU_L3_SGL.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#501\n",
    "#MUL 3 InceptionResNetV2\n",
    "# from keras.applications import InceptionV3\n",
    "# from keras.applications import Xception\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "f3_base_2 = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(299,299,3))  \n",
    "# for layer in f3_base.layers:\n",
    "#     layer.trainable = False \n",
    "f3_x_2 = f3_base_2.output\n",
    "f3_x_2 = GlobalAveragePooling2D()(f3_x_2)\n",
    "\n",
    "#Regularization with noise\n",
    "f3_x_2 = GaussianNoise(0.1)(f3_x_2)\n",
    "\n",
    "f3_x_2 = Dense(2048, activation='relu')(f3_x_2)\n",
    "f3_x_2 = Dense(10, activation='softmax')(f3_x_2)\n",
    "model_3_2 = Model(inputs=[f3_base_2.input],outputs=[f3_x_2])\n",
    "\n",
    "print(model_3_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_2.load_weights(os.path.join('checkpoints', 'Cifar10_IcpResNetV2_LRG299_Mul_STD_L2_SGL.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#501\n",
    "#MUL 3 InceptionResNetV2\n",
    "# from keras.applications import InceptionV3\n",
    "# from keras.applications import Xception\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, Reshape, Concatenate, Bidirectional\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "f3_base_3 = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(299,299,3))  \n",
    "# for layer in f3_base.layers:\n",
    "#     layer.trainable = False \n",
    "f3_x_3 = f3_base_3.output\n",
    "f3_x_3 = GlobalAveragePooling2D()(f3_x_3)\n",
    "\n",
    "#ADV\n",
    "f3_x_3 = Reshape([1,1536])(f3_x_3)  \n",
    "f3_x_3 = Bidirectional(LSTM(1024, \n",
    "                                 return_sequences=False, \n",
    "#                                  dropout=0.8\n",
    "                                ),\n",
    "                            input_shape=[1,1536],\n",
    "                            merge_mode='concat')(f3_x_3)\n",
    "\n",
    "#Regularization with noise\n",
    "f3_x_3 = GaussianNoise(0.1)(f3_x_3)\n",
    "\n",
    "f3_x_3 = Dense(2048, activation='relu')(f3_x_3)\n",
    "f3_x_3 = Dense(10, activation='softmax')(f3_x_3)\n",
    "model_3_3 = Model(inputs=[f3_base_3.input],outputs=[f3_x_3])\n",
    "\n",
    "print(model_3_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_3.load_weights(os.path.join('checkpoints', 'Cifar10_IcpResNetV2_LRG299_Mul_LSTM_L2_SGL.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_3_1.layers):\n",
    "    layer.name = layer.name + 'grp1_m1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_3_2.layers):\n",
    "    layer.name = layer.name + 'grp1_m2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_3_3.layers):\n",
    "    layer.name = layer.name + 'grp1_m3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 models (2mul + 1 +1)\n",
    "# 501\n",
    "# pre-trainned weight\n",
    "# get pretrained [-1] layer\n",
    "\n",
    "# from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, LeakyReLU, Dropout\n",
    "#from keras.applications.inception_v3 import preprocess_input\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "f1_mul_x = model_3_1.layers[-1].output \n",
    "f2_mul_x = model_3_2.layers[-1].output\n",
    "f3_mul_x = model_3_3.layers[-1].output\n",
    "\n",
    "x = concatenate([f1_mul_x, f2_mul_x, f3_mul_x])\n",
    "\n",
    "x = Dense(4096)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "model_mul = Model(inputs=[model_3_1.get_input_at(0),model_3_2.get_input_at(0),model_3_3.get_input_at(0)],outputs=[x])\n",
    "\n",
    "print(model_mul.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_mul.layers[:-4]:\n",
    "    layer.trainable=False\n",
    "# for layer in model.layers[87:]:\n",
    "#     layer.trainable=True)\n",
    "for i,layer in enumerate(model_mul.layers):\n",
    "    print(i,layer.name,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generators for three inputs\n",
    "def train_generator_three_img(X1, batch_size):\n",
    "    gen = ImageDataGenerator(    \n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,##\n",
    "    #     brightness_range=[0.5, 1.5],##\n",
    "        channel_shift_range=10,##\n",
    "        fill_mode='nearest')\n",
    "    \n",
    "    genX1 = gen.flow_from_directory(X1, batch_size=batch_size, seed=1, target_size = (299, 299))    \n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        yield [X1i[0], X1i[0], X1i[0]], X1i[1]\n",
    "        \n",
    "def valid_generator_three_img(X1, batch_size):\n",
    "    gen = ImageDataGenerator()\n",
    "    \n",
    "    # shuffle = False\n",
    "    genX1 = gen.flow_from_directory(X1, batch_size=batch_size, shuffle = False, target_size = (299, 299))        \n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        yield [X1i[0], X1i[0], X1i[0]], X1i[1]    \n",
    "    \n",
    "def test_generator_three_img(X1, batch_size):\n",
    "    gen = ImageDataGenerator(\n",
    "#         preprocessing_function=preprocess_input\n",
    "    )\n",
    "\n",
    "    genX1 = gen.flow_from_directory(X1, batch_size=batch_size, shuffle=False, target_size = (299, 299))        \n",
    "    while True:\n",
    "        X1i_0, X1i_1 = next(genX1)\n",
    "        yield [X1i_0, X1i_0, X1i_0]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Groups\n",
    "#Split training and validation\n",
    "#Using Expert Data\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "# from keras.utils import multi_gpu_model\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import multiprocessing\n",
    "\n",
    "## fix for multi_gpu_model prediction time longer\n",
    "from keras.layers import Lambda, concatenate\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "NUM_GPU = 4\n",
    "batch_size = 200\n",
    "#Using multiple models if more than 1 GPU\n",
    "if NUM_GPU != 1:\n",
    "    gpu_model_mul = multi_gpu_model(model_mul, gpus=NUM_GPU)\n",
    "\n",
    "epochs = 10##!!!\n",
    "lr = 1e-4\n",
    "decay = lr/epochs\n",
    "optimizer = Adam(lr=lr, decay=decay)\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    gpu_model_mul.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "valid_set = test_datagen.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "# model_txt = 'bi-lstm'\n",
    "# # Helper: Save the model.\n",
    "savedfilename = os.path.join('checkpoints', 'Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1.pkl')\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_acc', value=0.99, verbose=1)\n",
    "savecheckpoint = SaveCheckPoint(model_mul, savedfilename)\n",
    "\n",
    "# # Helper: TensorBoard\n",
    "# tb = TensorBoard(log_dir=os.path.join('cifar10_output', 'logs', model_txt))\n",
    "\n",
    "# # Helper: Save results.\n",
    "# timestamp = time.time()\n",
    "# csv_logger = CSVLogger(os.path.join('cifar10_output', 'logs', model_txt + '-' + 'training-' + \\\n",
    "#     str(timestamp) + '.log'))\n",
    "\n",
    "step_size_train=ceil(train_set.n/train_set.batch_size)\n",
    "step_size_valid=ceil(valid_set.n/valid_set.batch_size)\n",
    "# step_size_test=ceil(testing_set.n//testing_set.batch_size)\n",
    "\n",
    "result = gpu_model_mul.fit_generator(\n",
    "    generator = train_generator_three_img('train_resized_299',batch_size=batch_size), \n",
    "    steps_per_epoch = step_size_train,\n",
    "    validation_data = valid_generator_three_img('test_resized_299',batch_size=batch_size),\n",
    "    validation_steps = step_size_valid,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "#     callbacks=[tb, csv_logger, savecheckpoint, earlystopping], \n",
    "    callbacks=[savecheckpoint, earlystopping], \n",
    "    verbose=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pickle to load model weights\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "pklfile= os.path.join('checkpoints', 'Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1.pkl')\n",
    "\n",
    "f= open(pklfile, 'rb')     #Python 3                 \n",
    "weigh= pickle.load(f);                \n",
    "f.close();\n",
    "\n",
    "# #use set_weights to load the modelweights into the model architecture\n",
    "# NUM_GPU = 4\n",
    "# #Using multiple models if more than 1 GPU\n",
    "# if NUM_GPU != 1:\n",
    "#     model_mul = multi_gpu_model(model_2, gpus=NUM_GPU)\n",
    "\n",
    "gpu_model_mul.set_weights(weigh)\n",
    "\n",
    "m3_sgl = gpu_model_mul.layers[-2]\n",
    "\n",
    "#get weight from single gpu\n",
    "weigh= m3_sgl.get_weights()\n",
    "\n",
    "#now, use pickle to save model weights, instead of .h5\n",
    "#for heavy model architectures, .h5 file is unsupported.\n",
    "pklfile= os.path.join('checkpoints', 'Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1_SGL.pkl')\n",
    "\n",
    "fpkl= open(pklfile, 'wb') #Python 3\n",
    "pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "fpkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change test_generator\n",
    "import numpy as np\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    predict1=gpu_model_mul.predict_generator(test_generator_three_img('test_resized_299',batch_size), steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "# else:\n",
    "#     predict1=model.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "    \n",
    "predicted_class_indices=np.argmax(predict1,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions1 = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"file_name\":filenames,\n",
    "                      \"predicted1\":predictions1,\n",
    "                      })\n",
    "results.to_csv('Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1_SGL.csv')\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir pred_npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('pred_npy','Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1.npy'), predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1_SGL.csv /home/bribeiro/Phong/Nat19/Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1_SGL.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET 1\n",
    "\n",
    "from classification_models.keras import Classifiers\n",
    "from keras.models import Model\n",
    "# from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, SimpleRNN, Reshape, Concatenate,Bidirectional\n",
    "# from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "senet, preprocess_input = Classifiers.get('seresnext101')\n",
    "\n",
    "f2_base = senet(input_shape=(299,299,3), weights='imagenet', include_top=False)\n",
    "f2_x = GlobalAveragePooling2D()(f2_base.output)\n",
    "\n",
    "#ADV Model\n",
    "# f2_x = Reshape([1,2048])(f2_x)  \n",
    "# f2_x = SimpleRNN(2048, \n",
    "#             return_sequences=False,                       \n",
    "# #             dropout=0.8                                     \n",
    "#             input_shape=[1,2048])(f2_x)\n",
    "\n",
    "#Regularization with noise\n",
    "f2_x = GaussianNoise(0.1)(f2_x)\n",
    "\n",
    "f2_x = Dense(2048, activation='relu')(f2_x)\n",
    "f2_x = Dense(10, activation='softmax')(f2_x)\n",
    "model_2 = Model(inputs=[f2_base.input],outputs=[f2_x])\n",
    "\n",
    "print(model_2.summary())\n",
    "# # print(f2_base.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 - SeResNext101\n",
    "#use pickle to load model weights\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# load to multi gpu\n",
    "NUM_GPU = 4\n",
    "model_mul_2 = multi_gpu_model(model_2, gpus=NUM_GPU)\n",
    "\n",
    "#load model the saved weights\n",
    "pklfile= os.path.join('checkpoints', 'Cifar10_SEResNext101_LRG299_Mul_STD_L3.pkl')\n",
    "\n",
    "f= open(pklfile, 'rb')     #Python 3                 \n",
    "weigh= pickle.load(f);                \n",
    "f.close();\n",
    "\n",
    "#set the saved weight to mul-gpus model\n",
    "model_mul_2.set_weights(weigh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time, os\n",
    "from math import ceil\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    predict1=model_mul_2.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "# else:\n",
    "#     predict1=model.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "    \n",
    "predicted_class_indices=np.argmax(predict1,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions1 = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"file_name\":filenames,\n",
    "                      \"predicted1\":predictions1,\n",
    "                      })\n",
    "results.to_csv('Cifar10_SeResNet101_MulModels299_STD_L3.csv')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('pred_npy','Cifar10_SEResNext101_LRG299_L3.npy'), predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set 1\n",
    "#MUL 1 - Xception\n",
    "\n",
    "# from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, Reshape, Bidirectional\n",
    "# from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications.xception import preprocess_input\n",
    "\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "f1_base = Xception(weights='imagenet', include_top=False, input_shape=(299,299,3))  \n",
    "f1_x = f1_base.output\n",
    "f1_x = GlobalAveragePooling2D()(f1_x)\n",
    "\n",
    "# f1_x = Reshape([1,2048])(f1_x)  \n",
    "# f1_x = Bidirectional(LSTM(1024, \n",
    "#                                  return_sequences=False, \n",
    "# #                                  dropout=0.8\n",
    "#                                 ),\n",
    "#                             input_shape=[1,2048],\n",
    "#                             merge_mode='concat')(f1_x)\n",
    "\n",
    "#Regularization with noise\n",
    "f1_x = GaussianNoise(0.1)(f1_x)\n",
    "\n",
    "f1_x = Dense(2048, activation='relu')(f1_x)\n",
    "f1_x = Dense(10, activation='softmax')(f1_x)\n",
    "model_1 = Model(inputs=[f1_base.input],outputs=[f1_x])\n",
    "\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using multiple models if more than 1 GPU\n",
    "NUM_GPU = 4\n",
    "if NUM_GPU != 1:\n",
    "    model_mul_1 = multi_gpu_model(model_1, gpus=NUM_GPU)\n",
    "\n",
    "model_mul_1.load_weights(os.path.join('checkpoints', 'Cifar10_Xception_LRG299_Mul_STD_L2.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time, os\n",
    "from math import ceil\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    predict1=model_mul_1.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "# else:\n",
    "#     predict1=model.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "    \n",
    "predicted_class_indices=np.argmax(predict1,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions1 = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"file_name\":filenames,\n",
    "                      \"predicted1\":predictions1,\n",
    "                      })\n",
    "results.to_csv('Cifar10_Xception_MulModels299_STD_L2.csv')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('pred_npy','Cifar10_Xception_LRG299_L2.npy'), predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pred3_1 = np.load(os.path.join('pred_npy','Cifar10_Xception_LRG299_L2.npy'))\n",
    "mean_pred3_2 = np.load(os.path.join('pred_npy','Cifar10_SEResNext101_LRG299_L3.npy'))\n",
    "mean_pred3_3 = np.load(os.path.join('pred_npy','Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1.npy'))\n",
    "mean_pred3 = (mean_pred3_1+mean_pred3_2+mean_pred3_3)/3\n",
    "print(mean_pred3[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_pred3,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1211_v1.csv')\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1211_v1.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1211_v1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mean_pred3_1 = np.load(os.path.join('pred_npy','Cifar10_Xception_MulModels299_GRU_L3.npy'))\n",
    "mean_pred3_2 = np.load(os.path.join('pred_npy','Cifar10_SEResNext101_LRG299_L3.npy'))\n",
    "mean_pred3_3 = np.load(os.path.join('pred_npy','Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1.npy'))\n",
    "mean_pred3 = (mean_pred3_1+mean_pred3_2+mean_pred3_3)/3\n",
    "print(mean_pred3[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_pred3,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1211_v2.csv')\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1211_v2.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1211_v2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = mean_pred3_1\n",
    "a2 = mean_pred3_2\n",
    "a3 = mean_pred3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a1)\n",
    "print(mean_pred3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(a1[0], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(a1[0], axis=0) \n",
    "for idx in range(10):\n",
    "    if (idx != idx_max):\n",
    "        a1[0][idx] = 0\n",
    "    else:\n",
    "        a1[0][idx] = 1\n",
    "\n",
    "print(a1[0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a1)):\n",
    "    i_max = np.argmax(a1[i], axis=0)\n",
    "    for idx in range(10):\n",
    "        if idx != i_max:\n",
    "            a1[i][idx] = 0\n",
    "        else:\n",
    "            a1[i][idx] = 1            \n",
    "print(a1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a2)):\n",
    "    i_max = np.argmax(a2[i], axis=0)\n",
    "    for idx in range(10):\n",
    "        if idx != i_max:\n",
    "            a2[i][idx] = 0\n",
    "        else:\n",
    "            a2[i][idx] = 1             \n",
    "print(a2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a3)):\n",
    "    i_max = np.argmax(a3[i], axis=0)\n",
    "    for idx in range(10):\n",
    "        if idx != i_max:\n",
    "            a3[i][idx] = 0\n",
    "        else:\n",
    "            a3[i][idx] = 1             \n",
    "print(a3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_a = (a1+a2+a3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_a,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1211_v4.csv')\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1211_v4.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1211_v4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(a1[0], axis=0) \n",
    "max_val = a1[0][idx_max]\n",
    "a1[0][idx_max] = 0\n",
    "idx_max2 = np.argmax(a1[0], axis=0) \n",
    "\n",
    "for idx in range(10):\n",
    "    if (idx != idx_max2):\n",
    "        a1[0][idx] = 0\n",
    "\n",
    "a1[0][idx_max] = max_val\n",
    "\n",
    "print(a1[0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a1)):\n",
    "    idx_max = np.argmax(a1[i], axis=0) \n",
    "    max_val = a1[i][idx_max]\n",
    "    a1[i][idx_max] = 0\n",
    "    idx_max2 = np.argmax(a1[i], axis=0) \n",
    "\n",
    "    for idx in range(10):\n",
    "        if (idx != idx_max2):\n",
    "            a1[i][idx] = 0\n",
    "\n",
    "    a1[i][idx_max] = max_val\n",
    "\n",
    "print(a1[0:50])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9999321e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 6.2717859e-06 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 5.3751616e-09 0.0000000e+00]\n",
      " [9.9999833e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6641090e-06 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 5.6560179e-10 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 6.5428336e-11 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 1.0123520e-10 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 1.2328574e-09 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 2.1723653e-10 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 3.3746661e-09 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 5.0751686e-10 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(a2)):\n",
    "    idx_max = np.argmax(a2[i], axis=0) \n",
    "    max_val = a2[i][idx_max]\n",
    "    a2[i][idx_max] = 0\n",
    "    idx_max2 = np.argmax(a2[i], axis=0) \n",
    "\n",
    "    for idx in range(10):\n",
    "        if (idx != idx_max2):\n",
    "            a2[i][idx] = 0\n",
    "\n",
    "    a2[i][idx_max] = max_val\n",
    "\n",
    "print(a2[0:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a3)):\n",
    "    idx_max = np.argmax(a3[i], axis=0) \n",
    "    max_val = a3[i][idx_max]\n",
    "    a3[i][idx_max] = 0\n",
    "    idx_max2 = np.argmax(a3[i], axis=0) \n",
    "\n",
    "    for idx in range(10):\n",
    "        if (idx != idx_max2):\n",
    "            a3[i][idx] = 0\n",
    "\n",
    "    a3[i][idx_max] = max_val\n",
    "\n",
    "print(a3[0:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_a =(a1+a2+a3)/3\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_a,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1211_v5.csv')\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1211_v5.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1211_v5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp /home/bribeiro/Phong/Nat19/Cifar10_Eff_LRGB4224_Mul.npy pred_npy/Cifar10_Eff_LRGB4224_Mul.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mean_pred3_4 = np.load(os.path.join('pred_npy','Cifar10_Eff_LRGB4224_Mul.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4 = mean_pred3_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a4)):\n",
    "    idx_max = np.argmax(a4[i], axis=0) \n",
    "    max_val = a4[i][idx_max]\n",
    "    a4[i][idx_max] = 0\n",
    "    idx_max4 = np.argmax(a4[i], axis=0) \n",
    "\n",
    "    for idx in range(10):\n",
    "        if (idx != idx_max4):\n",
    "            a4[i][idx] = 0\n",
    "\n",
    "    a4[i][idx_max] = max_val\n",
    "\n",
    "print(a4[0:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_a = (a2+a3+a4)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_a,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1411_v1.csv')\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1411_v1.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1411_v1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_a = (a1+a2+a3+a4)/4\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_a,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1411_v3.csv')\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1411_v3.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1411_v3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.98876572e-01 1.06719475e-04 1.47624305e-04 1.06542582e-04\n",
      "  1.17790914e-04 1.20928911e-04 1.17569034e-04 1.05496845e-04\n",
      "  1.97017565e-04 1.03772152e-04]\n",
      " [9.98881161e-01 1.06688065e-04 1.45969389e-04 1.06549182e-04\n",
      "  1.17798169e-04 1.20936718e-04 1.17576616e-04 1.05503765e-04\n",
      "  1.94046515e-04 1.03778664e-04]\n",
      " [9.98880625e-01 1.06690895e-04 1.45967075e-04 1.06549356e-04\n",
      "  1.17796997e-04 1.20935350e-04 1.17575102e-04 1.05502455e-04\n",
      "  1.94600216e-04 1.03781982e-04]\n",
      " [9.98881280e-01 1.06680782e-04 1.45959479e-04 1.06542291e-04\n",
      "  1.17790558e-04 1.20928620e-04 1.17568685e-04 1.05496641e-04\n",
      "  1.94018692e-04 1.03771563e-04]\n",
      " [9.98881161e-01 1.06680985e-04 1.45961953e-04 1.06576830e-04\n",
      "  1.17790732e-04 1.20928766e-04 1.17568874e-04 1.05496751e-04\n",
      "  1.94019216e-04 1.03771767e-04]\n",
      " [9.98881280e-01 1.06680985e-04 1.45959537e-04 1.06542408e-04\n",
      "  1.17790674e-04 1.20928737e-04 1.17568852e-04 1.05496751e-04\n",
      "  1.94018954e-04 1.03771767e-04]\n",
      " [9.98881280e-01 1.06680913e-04 1.45959755e-04 1.06542291e-04\n",
      "  1.17790616e-04 1.20928678e-04 1.17568736e-04 1.05496641e-04\n",
      "  1.94019463e-04 1.03771679e-04]\n",
      " [9.98881280e-01 1.06681378e-04 1.45960104e-04 1.06542895e-04\n",
      "  1.17791285e-04 1.20929362e-04 1.17569463e-04 1.05497136e-04\n",
      "  1.94020802e-04 1.03772261e-04]\n",
      " [9.98880804e-01 1.06713407e-04 1.46106875e-04 1.06577550e-04\n",
      "  1.17828451e-04 1.20965669e-04 1.17607480e-04 1.05530926e-04\n",
      "  1.94085296e-04 1.03805738e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# mean_pred3_1 = np.load(os.path.join('pred_npy','Cifar10_Xception_MulModels299_GRU_L3.npy'))\n",
    "mean_pred3_2 = np.load(os.path.join('pred_npy','Cifar10_SEResNext101_LRG299_L3.npy'))\n",
    "mean_pred3_3 = np.load(os.path.join('pred_npy','Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1.npy'))\n",
    "mean_pred3_5 = np.load(os.path.join('pred_npy','Cifar10_Eff_B5_345_L2.npy'))\n",
    "\n",
    "mean_pred3 = (mean_pred3_5+mean_pred3_2+mean_pred3_3)/3\n",
    "print(mean_pred3[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/aeroplane_s_000002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/aeroplane_s_000040.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/aeroplane_s_000045.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/aeroplane_s_000063.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/airbus_s_000009.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0/airbus_s_000030.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0/airbus_s_000049.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0/airbus_s_000099.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0/airbus_s_000108.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0/airbus_s_000131.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0/airbus_s_000136.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0/airbus_s_000156.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0/airbus_s_000170.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0/airbus_s_000198.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0/airbus_s_000211.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0/airbus_s_000232.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0/airbus_s_000234.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0/airbus_s_000249.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0/airbus_s_000257.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0/airbus_s_000301.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id predicted\n",
       "0   0/aeroplane_s_000002.png         0\n",
       "1   0/aeroplane_s_000040.png         0\n",
       "2   0/aeroplane_s_000045.png         0\n",
       "3   0/aeroplane_s_000063.png         0\n",
       "4      0/airbus_s_000009.png         0\n",
       "5      0/airbus_s_000030.png         0\n",
       "6      0/airbus_s_000049.png         0\n",
       "7      0/airbus_s_000099.png         0\n",
       "8      0/airbus_s_000108.png         0\n",
       "9      0/airbus_s_000131.png         0\n",
       "10     0/airbus_s_000136.png         0\n",
       "11     0/airbus_s_000156.png         0\n",
       "12     0/airbus_s_000170.png         0\n",
       "13     0/airbus_s_000198.png         0\n",
       "14     0/airbus_s_000211.png         0\n",
       "15     0/airbus_s_000232.png         0\n",
       "16     0/airbus_s_000234.png         0\n",
       "17     0/airbus_s_000249.png         0\n",
       "18     0/airbus_s_000257.png         0\n",
       "19     0/airbus_s_000301.png         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_a = (a1+a2+a3+a4)/4\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_pred3,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1511_v1.csv')\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1511_v1.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1511_v1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a5 = mean_pred3_5\n",
    "a2 = mean_pred3_2\n",
    "a3 = mean_pred3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999932 0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.       ]\n",
      " [1.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(a2)):\n",
    "#     idx_max = np.argmax(a2[i], axis=0) \n",
    "#     max_val = a2[i][idx_max]\n",
    "#     a2[i][idx_max] = 0\n",
    "    idx_max2 = np.argmax(a2[i], axis=0) \n",
    "\n",
    "    for idx in range(10):\n",
    "        if (idx != idx_max2):\n",
    "            a2[i][idx] = 0\n",
    "\n",
    "#     a2[i][idx_max] = max_val\n",
    "\n",
    "print(a2[0:2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9966438  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.99664354 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(a3)):\n",
    "#     idx_max = np.argmax(a3[i], axis=0) \n",
    "#     max_val = a3[i][idx_max]\n",
    "#     a3[i][idx_max] = 0\n",
    "    idx_max3 = np.argmax(a3[i], axis=0) \n",
    "\n",
    "    for idx in range(10):\n",
    "        if (idx != idx_max3):\n",
    "            a3[i][idx] = 0\n",
    "\n",
    "#     a3[i][idx_max] = max_val\n",
    "\n",
    "print(a3[0:2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999927 0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.       ]\n",
      " [1.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(a5)):\n",
    "#     idx_max = np.argmax(a3[i], axis=0) \n",
    "#     max_val = a3[i][idx_max]\n",
    "#     a3[i][idx_max] = 0\n",
    "    idx_max5 = np.argmax(a5[i], axis=0) \n",
    "\n",
    "    for idx in range(10):\n",
    "        if (idx != idx_max5):\n",
    "            a5[i][idx] = 0\n",
    "\n",
    "#     a3[i][idx_max] = max_val\n",
    "\n",
    "print(a5[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/aeroplane_s_000002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/aeroplane_s_000040.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/aeroplane_s_000045.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/aeroplane_s_000063.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/airbus_s_000009.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0/airbus_s_000030.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0/airbus_s_000049.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0/airbus_s_000099.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0/airbus_s_000108.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0/airbus_s_000131.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0/airbus_s_000136.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0/airbus_s_000156.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0/airbus_s_000170.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0/airbus_s_000198.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0/airbus_s_000211.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0/airbus_s_000232.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0/airbus_s_000234.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0/airbus_s_000249.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0/airbus_s_000257.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0/airbus_s_000301.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id predicted\n",
       "0   0/aeroplane_s_000002.png         0\n",
       "1   0/aeroplane_s_000040.png         0\n",
       "2   0/aeroplane_s_000045.png         0\n",
       "3   0/aeroplane_s_000063.png         0\n",
       "4      0/airbus_s_000009.png         0\n",
       "5      0/airbus_s_000030.png         0\n",
       "6      0/airbus_s_000049.png         0\n",
       "7      0/airbus_s_000099.png         0\n",
       "8      0/airbus_s_000108.png         0\n",
       "9      0/airbus_s_000131.png         0\n",
       "10     0/airbus_s_000136.png         0\n",
       "11     0/airbus_s_000156.png         0\n",
       "12     0/airbus_s_000170.png         0\n",
       "13     0/airbus_s_000198.png         0\n",
       "14     0/airbus_s_000211.png         0\n",
       "15     0/airbus_s_000232.png         0\n",
       "16     0/airbus_s_000234.png         0\n",
       "17     0/airbus_s_000249.png         0\n",
       "18     0/airbus_s_000257.png         0\n",
       "19     0/airbus_s_000301.png         0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_a = (a5+a2+a3)/3\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_a,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1511_v2.csv')\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1511_v2.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1511_v2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.98878717e-01 1.06719403e-04 1.46113845e-04 1.06542517e-04\n",
      "  1.17790849e-04 1.20928911e-04 1.17569019e-04 1.05496838e-04\n",
      "  1.96383786e-04 1.03771956e-04]\n",
      " [9.98881161e-01 1.06688065e-04 1.45969374e-04 1.06549182e-04\n",
      "  1.17798169e-04 1.20936718e-04 1.17576616e-04 1.05503765e-04\n",
      "  1.94046515e-04 1.03778664e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# mean_pred3_1 = np.load(os.path.join('pred_npy','Cifar10_Xception_MulModels299_GRU_L3.npy'))\n",
    "mean_pred3_2 = np.load(os.path.join('pred_npy','Cifar10_SEResNext101_LRG299_L3.npy'))\n",
    "mean_pred3_3 = np.load(os.path.join('pred_npy','Cifar10_LRG299_Mul_Pretrained_V3_3MIcpRes_STD_L1.npy'))\n",
    "mean_pred3_5_2 = np.load(os.path.join('pred_npy','Cifar10_Eff_B5_345_L2_TTA3.npy'))\n",
    "\n",
    "mean_pred3 = (mean_pred3_5_2+mean_pred3_2+mean_pred3_3)/3\n",
    "print(mean_pred3[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/aeroplane_s_000002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/aeroplane_s_000040.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/aeroplane_s_000045.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/aeroplane_s_000063.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/airbus_s_000009.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id predicted\n",
       "0  0/aeroplane_s_000002.png         0\n",
       "1  0/aeroplane_s_000040.png         0\n",
       "2  0/aeroplane_s_000045.png         0\n",
       "3  0/aeroplane_s_000063.png         0\n",
       "4     0/airbus_s_000009.png         0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_a = (a5+a2+a3)/3\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from keras_applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_pred3,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_MulModels299_AVG_Assembe_1511_v4.csv')\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_MulModels299_AVG_Assembe_1511_v4.csv /home/bribeiro/Phong/Nat19/Cifar10_MulModels299_AVG_Assembe_1511_v4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 1.12.0 has requirement keras-applications>=1.0.6, but you have keras-applications 1.0.2.\n",
      "tensorflow 1.12.0 has requirement keras-preprocessing>=1.0.5, but you have keras-preprocessing 1.0.1.\n",
      "tensorflow 1.12.0 has requirement tensorboard<1.13.0,>=1.12.0, but you have tensorboard 1.8.0.\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip check numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 1.12.0 has requirement keras-applications>=1.0.6, but you have keras-applications 1.0.2.\n",
      "tensorflow 1.12.0 has requirement keras-preprocessing>=1.0.5, but you have keras-preprocessing 1.0.1.\n",
      "tensorflow 1.12.0 has requirement tensorboard<1.13.0,>=1.12.0, but you have tensorboard 1.8.0.\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip check Keras-Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras; print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow-gpu\n",
      "Version: 1.8.0\n",
      "Summary: TensorFlow helps the tensors flow\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: opensource@google.com\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python2.7/dist-packages\n",
      "Requires: astor, protobuf, gast, tensorboard, six, wheel, absl-py, backports.weakref, termcolor, enum34, numpy, grpcio, mock\n",
      "Required-by: \n",
      "\u001b[33mYou are using pip version 18.0, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
